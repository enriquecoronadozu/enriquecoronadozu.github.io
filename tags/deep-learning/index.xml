<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Academic</title>
    <link>https://enriquecoronadozu.github.io/tags/deep-learning/index.xml</link>
    <description>Recent content in Deep Learning on Academic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Enrique Coronado</copyright>
    <atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Deep Learning: Introduction to Recurrent Neural Networks</title>
      <link>https://enriquecoronadozu.github.io/post/2017-02-01-Deep%20Learning/</link>
      <pubDate>Wed, 01 Feb 2017 10:00:00 +0000</pubDate>
      
      <guid>https://enriquecoronadozu.github.io/post/2017-02-01-Deep%20Learning/</guid>
      <description>&lt;p&gt;This post gives and introduction to Recurrent Neural Networks (RNN) &lt;/p&gt;

&lt;h2 id=&#34;notations&#34;&gt;Notations:&lt;/h2&gt;

&lt;h3 id=&#34;list-of-abbrevations&#34;&gt;List of abbrevations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;RNN (Recurrent Neural Networks)&lt;/li&gt;
&lt;li&gt;FFN (Feed-forward network)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;list-of-symbols&#34;&gt;List of symbols&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$T$,  length of a sequence&lt;/li&gt;
&lt;li&gt;$\tau$ length of a minibatch of a sequence&lt;/li&gt;
&lt;li&gt;$\mathbf{x} = {x_1, x_2, \dots, x_n }$, vector set&lt;/li&gt;
&lt;li&gt;$\mathcal{X} = {x_1, x_2, \dots, x_T }$,  sequence&lt;/li&gt;
&lt;li&gt;$x$,  element of a set or of a sequence&lt;/li&gt;
&lt;li&gt;$\mathbf{h}$ state in the hidden units of the network&lt;/li&gt;
&lt;li&gt;$t$,    time step&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;theory&#34;&gt;Theory:&lt;/h2&gt;

&lt;h3 id=&#34;recurrent-neural-networks-rnn&#34;&gt;Recurrent Neural Networks (RNN)&lt;/h3&gt;

&lt;p&gt;Type of Neural Network for processing &lt;strong&gt;secuential data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Main ideas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Are called recurrent becuase perform the same task for every element of a sequence&lt;/li&gt;
&lt;li&gt;Have a memory that capture the information calculated before&lt;/li&gt;
&lt;li&gt;Use cycles to represent the influnce of past values at the current value&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;comparison-between-traditional-neural-networks-and-rnn&#34;&gt;Comparison between traditional Neural Networks and RNN&lt;/h3&gt;

&lt;p&gt;Traditional Neural Networks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Outputs are independent of previous computations (no good idea for prediction)&lt;/li&gt;
&lt;li&gt;Inputs are &lt;strong&gt;sets&lt;/strong&gt; of features of fixed length&lt;/li&gt;
&lt;li&gt;Diferent parameters (weights) for each input feature&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recurrent Neural Networks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Outputs are dependents of previous computations (each output is a function of the previous outputs)&lt;/li&gt;
&lt;li&gt;Inputs are &lt;strong&gt;sequential&lt;/strong&gt; information (can be of variable length)&lt;/li&gt;
&lt;li&gt;Parameter sharing: share the same weights across several time steps. This make possible to apply a RNN model to examples of diferent lengths.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Note: See appendix A to know more about sequences&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;feed-forward-network-ffn&#34;&gt;Feed-forward network (FFN)&lt;/h3&gt;

&lt;p&gt;Given a vector set $\mathbf{x} = {x_1, x_2, \dots , x_j, \dots, x_n }$ of $n$ inputs features, the output $i$ of a FFN is given by:&lt;/p&gt;

&lt;p&gt;\begin{equation}
y&lt;em&gt;i = f \left( \sum&lt;/em&gt;{j}^{m} W_{ij}  x_j + b_i \right)
\end{equation}&lt;/p&gt;

&lt;p&gt;Which represent the sum of each element $j$ of the vector of inputs multiplied by the weigth $(i,j)$ in the neuron $i$ plus the bias value in the neuron $i$, and $f$ represent the output function (also called activation function).&lt;/p&gt;

&lt;p&gt;Its matrix form is represeted as:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{y} = \mathbf{W} \mathbf{x} + \mathbf{b}
\end{equation}&lt;/p&gt;

&lt;p&gt;where $\mathbf{y}= { y_1, y_2, \dots, y_m }$ is the vector of outputs,  $\mathbf{W}$ a the matrix of weights, and $\mathbf{b} = { b_1, b_2, \dots, b_m }$ is a vector of bias.&lt;/p&gt;

&lt;h3 id=&#34;rnn-operation&#34;&gt;RNN operation:&lt;/h3&gt;

&lt;p&gt;Given a sequence $X = {x_1, x_2, \dots, x_T }$, a new state $\mathbf{h}$ in the hidden units of the RNN is given by the next recurrent function:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{h}_t = f&lt;em&gt;h\left(\mathbf{W}\mathbf{h}&lt;/em&gt;{t-1} + \mathbf{U}\mathbf{x}_{t} + \mathbf{b}_h \right)
\end{equation}&lt;/p&gt;

&lt;p&gt;The output $\mathbf{y}_t$ is computed as:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{y}_t = f_y \left(  \mathbf{V} \mathbf{h}_t + \mathbf{b}_y  \right)
\end{equation}&lt;/p&gt;

&lt;p&gt;Where $\mathbf{W}$, $\mathbf{U}$ and $\mathbf{V}$ are matrices of parameters. Also $\mathbf{b}_h$ and $\mathbf{b}_h$ are vectors of bias values and $f_h$ and $f_y$ are activation functions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RNN operates in mini batches of the $X$ sequence. Each minibatch can have diferent sequence length $\tau$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;references&#34;&gt;References:&lt;/h3&gt;

&lt;p&gt;Goodfellow, I., Bengio, Y., &amp;amp; Courville, A. (2016). Deep learning. MIT Press.&lt;/p&gt;

&lt;h2 id=&#34;example-1-simple-keras-implementation&#34;&gt;Example 1: Simple Keras implementation&lt;/h2&gt;

&lt;p&gt;In this example the well known pima dataset (&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes&#34; target=&#34;_blank&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes&lt;/a&gt;) will be used as example of a clasification task using RNN&lt;/p&gt;

&lt;h3 id=&#34;step-1-import-python-libraries-and-load-data-set&#34;&gt;Step 1: Import python libraries and load data set&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import libraries
from numpy import*
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Embedding, Activation, LSTM

# load pima indians dataset
dataset = loadtxt(&amp;quot;pima-indians-diabetes.csv&amp;quot;, delimiter=&amp;quot;,&amp;quot;)
# split into input (X) and output (Y) variables
X = dataset[:500,0:8]
Y = dataset[:500,8]

# For validation
X_val = dataset[500:600,0:8]
Y_val = dataset[500:600,8]

n_example,n_inputs = shape(X)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2-create-a-new-sequential-model&#34;&gt;Step 2: Create a new sequential model&lt;/h3&gt;

&lt;p&gt;We need to define:
- The firts layer
- The other hidden layers if needed
- The output layer&lt;/p&gt;

&lt;p&gt;The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first) needs to receive information about its input shape.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 2.1: Specification of a sequential model
model = Sequential()
# Step 2.2: Defintion of the first layer
#model.add(SimpleRNN(32, input_shape=(1,8)))
#model.add(Activation(&amp;quot;sigmoid&amp;quot;))
# Step 2.3 Definition of the uutput layer
# model.add(Dense(10, 1, activation = &amp;quot;sigmoid&amp;quot;))
#model.add(Dense(5))

# Firts layer
#model.add(Embedding(n_inputs, 500))
#model.add(SimpleRNN(3, input_dim = n_inputs))
#model.add(LSTM(3, input_dim = n_inputs))

#model.add(Masking(0, input_dim = n_inputs))
model.add(SimpleRNN(3, input_dim = n_inputs))

#Final
model.add(Dense(1, activation = &amp;quot;sigmoid&amp;quot;))



&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-3-compile-the-model&#34;&gt;Step 3: Compile the model&lt;/h3&gt;

&lt;p&gt;This can be done using the next function&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(loss=&#39;binary_crossentropy&#39;,
              optimizer=&#39;adam&#39;,
              metrics=[&#39;accuracy&#39;])

#model.compile(loss=&#39;mean_squared_error&#39;,
#              optimizer=&#39;adam&#39;,
#              metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-4-reshape-the-data&#34;&gt;Step 4: reshape the data&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trainX = reshape(X, (X.shape[0],1,X.shape[1]))
validateX = reshape(X_val, (X_val.shape[0],1,X_val.shape[1]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-5-fit-the-data&#34;&gt;Step 5: Fit the data&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
print(&#39;Training ...&#39;)
model.fit(trainX,Y, batch_size=10, nb_epoch=150, verbose=0)
print (&#39;Training complete&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Training ...
Training complete
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-5-evaluate-the-network&#34;&gt;Step 5: Evaluate the network&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scores = model.evaluate(validateX, Y_val)
print(model.metrics_names[1], scores[1]*100)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt; 32/100 [========&amp;gt;.....................] - ETA: 0s(&#39;acc&#39;, 68.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;references-1&#34;&gt;References:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://keras.io/getting-started/sequential-model-guide/#specifying-the-input-shape&#34; target=&#34;_blank&#34;&gt;https://keras.io/getting-started/sequential-model-guide/#specifying-the-input-shape&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&#34;a-what-is-a-sequence&#34;&gt;A:  What is a sequence?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;A sequence is an enumerated collection of objects (i.e list of elements that are in some order).&lt;/li&gt;
&lt;li&gt;The number of elements is called the length of the sequence, is represented in this tutorial as $T$&lt;/li&gt;
&lt;li&gt;The same elements can appear multiple times at different positions in the sequence.&lt;/li&gt;
&lt;li&gt;Unlike a set, order matters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference from: &lt;a href=&#34;https://en.wikipedia.org/wiki/Sequence&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Sequence&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additional theory:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sets: &lt;a href=&#34;https://en.wikipedia.org/wiki/Set_(mathematics&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Set_(mathematics&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Basic explanation of sets and sequences: &lt;a href=&#34;http://www.mathsisfun.com/algebra/sequences-series.html&#34; target=&#34;_blank&#34;&gt;http://www.mathsisfun.com/algebra/sequences-series.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Installation of Machine Learning libraries for Python</title>
      <link>https://enriquecoronadozu.github.io/post/2017-01-30-Installation%20of%20Keras/</link>
      <pubDate>Mon, 30 Jan 2017 10:00:00 +0000</pubDate>
      
      <guid>https://enriquecoronadozu.github.io/post/2017-01-30-Installation%20of%20Keras/</guid>
      <description>&lt;p&gt;This post contains the steps followed to perform the installation of the machine learning libraries used for my thesis. &lt;/p&gt;

&lt;h2 id=&#34;python-installation&#34;&gt;Python Installation&lt;/h2&gt;

&lt;p&gt;The python version used is 2.7 of 32-bit from the Anacoda distribution:
&lt;a href=&#34;https://www.continuum.io/downloads&#34; target=&#34;_blank&#34;&gt;https://www.continuum.io/downloads&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;update-pip-install&#34;&gt;Update pip install&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;python -m pip install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installation-of-machine-learning-libraries&#34;&gt;Installation of machine learning libraries&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# Used in MAC OS to solve the error: &amp;quot;HDF5 library version mismatched error&amp;quot;
conda install h5py

# Theano library
pip install theano

## TensorFlow library: Used only for MAC OS X and Linux
pip install tensorflow

# Keras library (In windows opening the CMD as administrator)
pip install keras
conda install mingw libpython
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;keras-backend&#34;&gt;Keras backend&lt;/h2&gt;

&lt;p&gt;Keras has two backend implementations (2017): TensorFlow and Theano. The default backend implementation is TensorFlow.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In Windows there is only the Python 3.5 version of TensorFlow. In order to use Keras + Python 2.7 in Windows, the next steps can be performed:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Go to &lt;code&gt;C:\Users\&amp;lt;user name&amp;gt;&lt;/code&gt; and open the folder .keras&lt;/li&gt;
&lt;li&gt;Open the file keras.json and change the lines as:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{
    &amp;quot;image_dim_ordering&amp;quot;: &amp;quot;th&amp;quot;,
    &amp;quot;epsilon&amp;quot;: 1e-07,
    &amp;quot;floatx&amp;quot;: &amp;quot;float32&amp;quot;,
    &amp;quot;backend&amp;quot;: &amp;quot;theano&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;More info in: &lt;a href=&#34;https://keras.io/backend/&#34; target=&#34;_blank&#34;&gt;https://keras.io/backend/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Biomechanical parameters estimation</title>
      <link>https://enriquecoronadozu.github.io/project/Biomechanical%20parameters%20estimation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://enriquecoronadozu.github.io/project/Biomechanical%20parameters%20estimation/</guid>
      <description>&lt;p&gt;&lt;p style=&#39;text-align: justify;&#39;&gt;
The estimation of human ankle’s mechanical impedance is an important tool used to gain insight on the mechanisms involved in postural stability. This knowledge is indispensable to understand how humans can regain balance against possible disturbances. The objetive of this project is to validate quiet standing muscle-tendon mathematical models. For this measurements using low-cost motion capture systems and
algorithms for the identification of muscle-tendons’ parameters are used. These parameters are particularly important to compare betweem healthy subjects and individuals who have been impaired as a consequence of injuries or neurological diseases, or are elderly and more prone to fall.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gesture-based Control</title>
      <link>https://enriquecoronadozu.github.io/project/Gesture-based%20control/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://enriquecoronadozu.github.io/project/Gesture-based%20control/</guid>
      <description>&lt;p&gt;Typical forms of communication with robots in industrial and laboratory environments are using devices such as keyboard, mouse, joystick and teach pendant. These forms of communication are generally considered unnatural methods and require prior training, which can be unpleasant and time consuming for novice user.
The objective of this project is to propose simpler or novel forms of communication that improve the usability and user experience of robots in real-world enviroments. Such simplification is possible with the adoption of natural interfaces, i.e., interfaces that enables users to interact with machines and computers in the same way they interact with their everyday environment.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
